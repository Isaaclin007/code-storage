{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want a big corpus to do NLP research?  How about all of Wikipedia?<br>\n",
    "https://corpus.byu.edu/wiki/ <br>\n",
    "This corpus contains approximately 2 billion words, and a vocabulary of approximately 400,000 words, which corresponds to 400,000 dimensions in the bag of words model.  This is a big space to deal with!  How might we reduce our space to a smaller dimension?\n",
    "\n",
    "A single word such as *computer* might be represented in bag of words as a vector like $(1,0,0,\\dots,0)$ and another word like *software* might be represented as a vector like $(0,1,0,0,\\dots,0)$.  The two vectors are orthogonal to each other, despite the fact that the words *computer* and *software* are related to each other.\n",
    "\n",
    "The technique of ***word embedding*** attempts to embed words into a continuous vector space of smaller dimension (say 50) in such a way their semantic relationships are preserved.<br>\n",
    "$$\\mathbb{Z_{\\geq 0}^{400,000} \\longrightarrow \\mathbb{R}^{50}}$$\n",
    "\n",
    "For example, we might hope that the vector corresponding to *computer* might be relatively similar to vector corresponding to *software*. \n",
    "\n",
    "Given an embedding scheme, we can either train it on our own corpus of data or we can use a pre-trained model.\n",
    "\n",
    "There are several available word embedding schemes that are publicly available.  One of the first to be created was the **Word2vec** scheme created (and patented) by Tomáš Mikolov of Google.  They have an archive consisting of code and pre-trained vectors. The latter was trained on Google News data and consists of 3,000,000 words and phrases embedded in 300 dimensions.<br>\n",
    "https://code.google.com/archive/p/word2vec/\n",
    "\n",
    "Another commonly used scheme created NLP researchers Manning, Pennington and Socher at Stanford Univesity is the **Global Vectors for Word Representation (GloVe)**, which also provides code vectors pre-trained on various corpora, such as the Wikipedia corpus and a Twitter corpus.<br>\n",
    "https://nlp.stanford.edu/projects/glove/<br>\n",
    "\n",
    "\n",
    "Stanford also provides a number of good NLP lectures that are available on YouTube.  For example,<br>\n",
    "https://www.youtube.com/watch?v=OQQ-W_63UgQ<br>\n",
    "is Christopher Manning's introductory lecture for Stanford's NLP class, and at the 55:30 mark he discusses word vectors.  And Manning's entire second lecture is on word vectors:<br>\n",
    "https://www.youtube.com/watch?v=ERibwqs9p38\n",
    "\n",
    "If 80 minute video lectures aren't your thing, how about a pair of 1 minute videos from Udacity?<br>\n",
    "https://www.youtube.com/watch?v=186HUTBQnpY<br>\n",
    "https://www.youtube.com/watch?v=xMwx2A_o5r4\n",
    "\n",
    "The GloVe file ```glove6B.zip``` provides four different pre-trained word embeddings for 400,000 words based on the Wikipedia corpus. The embeddings are in dimensions 50, 100, 200 and 300.  For simplicity, let's focus on the 50-dimensional embeddings, in the file ```glove.6B.50d.txt```.\n",
    "\n",
    "If we inspect ```glove.6B.50d.txt```, we see that it is a text file consisting of rows separted by line returns and commas, and each line is a word and 50 signed floating point numbers separated by spaces.\n",
    "\n",
    "```the 0.418 0.24968 -0.41242 0.1217 0.34527 -0.044457 -0.49688 -0.17862 -0.00066023 -0.6566 0.27843 -0.14767 -0.55677 0.14658 -0.0095095 0.011658 0.10204 -0.12792 -0.8443 -0.12181 -0.016801 -0.33279 -0.1552 -0.23131 -0.19181 -1.8823 -0.76746 0.099051 -0.42125 -0.19526 4.0071 -0.18594 -0.52287 -0.31681 0.00059213 0.0074449 0.17778 -0.15897 0.012041 -0.054223 -0.29871 -0.15749 -0.34758 -0.045637 -0.44251 0.18785 0.0027849 -0.18411 -0.11514 -0.78581\n",
    ",```\n",
    "\n",
    "Let's  open up the GloVe file for reading. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fin = open (\"glove.6B/glove.6B.50d.txt\",\"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's read in each of the lines into a list of lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = fin.readlines()\n",
    "fin.close()  # Closes the file and frees any system resources used by the open file.\n",
    "len(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's parse each line and create a dictionary of word vectors (which we'll store as ```numpy``` arrays)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "word2vec = {}\n",
    "for line in lines:\n",
    "    splitLine = line.split(\" \")\n",
    "    word = splitLine[0]\n",
    "    vector = np.array([float(x) for x in splitLine[1:51]])\n",
    "    word2vec[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.418, 0.24968, -0.41242, 0.1217, 0.34527, -0.044457, -0.49688, -0.17862, -0.00066023, -0.6566, 0.27843, -0.14767, -0.55677, 0.14658, -0.0095095, 0.011658, 0.10204, -0.12792, -0.8443, -0.12181, -0.016801, -0.33279, -0.1552, -0.23131, -0.19181, -1.8823, -0.76746, 0.099051, -0.42125, -0.19526, 4.0071, -0.18594, -0.52287, -0.31681, 0.00059213, 0.0074449, 0.17778, -0.15897, 0.012041, -0.054223, -0.29871, -0.15749, -0.34758, -0.045637, -0.44251, 0.18785, 0.0027849, -0.18411, -0.11514, -0.78581]\n"
     ]
    }
   ],
   "source": [
    "print(list(word2vec['the']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the Euclidean length of this vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.9678269047049"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(word2vec['the'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a functions that calculates the cosine similarity between two vectors and between two words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosDist(v1,v2):\n",
    "    cosine = np.dot(v1,v2) / ( np.linalg.norm(v1) * np.linalg.norm(v2) )\n",
    "    return(cosine)\n",
    "\n",
    "def wordDist(w1,w2):\n",
    "    v1 = word2vec[w1]\n",
    "    v2 = word2vec[w2]    \n",
    "    cosine = cosDist(v1,v2)\n",
    "    return(cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.519508383525476"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordDist(\"computer\",\"the\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8814993634710456"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordDist(\"computer\",\"software\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6757718666324486\n",
      "0.440908893106959\n",
      "0.1505738416811099\n",
      "-0.07746374002723987\n"
     ]
    }
   ],
   "source": [
    "print(wordDist(\"beyonce\",\"gaga\"))  # Beyonce and Lady Gaga are similar.\n",
    "print(wordDist(\"beyonce\",\"jay\"))\n",
    "print(wordDist(\"gaga\",\"jay\")) # and Jay (Z) is closer to Beyonce than to Lady Gaga.\n",
    "print(wordDist(\"beyonce\",\"naiman\")) # but Naiman and Beyonce are orthongal to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.82361796933357\n"
     ]
    }
   ],
   "source": [
    "cosineDict = {w:wordDist(\"king\",w) for w in word2vec}  # dictionary comprension\n",
    "print(cosineDict[\"prince\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0000000000000002, 'king'),\n",
       " (0.82361796933357, 'prince'),\n",
       " (0.7839043010964116, 'queen'),\n",
       " (0.7746230030635107, 'ii'),\n",
       " (0.7736247624872923, 'emperor'),\n",
       " (0.7667193954606585, 'son'),\n",
       " (0.7627150944065074, 'uncle'),\n",
       " (0.7542161124008465, 'kingdom'),\n",
       " (0.7539914268281643, 'throne'),\n",
       " (0.7492411846124971, 'brother')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(value,key) for (key,value) in cosineDict.items()], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings sometime preserve semantic relationships between words.  For example, consider the analogy:<br>\n",
    "\"`Man` is to `king` as `woman` is to $______________$\".<br>\n",
    "What term should go in the blank?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = word2vec['king'] - word2vec['man'] + word2vec['woman']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look for words that are similar to the above vector of `king` - `man` + `woman`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosineDict = {w:cosDist(v,word2vec[w]) for w in word2vec}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.8859834623625931, 'king'),\n",
       " (0.8609581258578942, 'queen'),\n",
       " (0.768451180089547, 'daughter'),\n",
       " (0.764069959135472, 'prince'),\n",
       " (0.7634970756412144, 'throne'),\n",
       " (0.7512728447426918, 'princess'),\n",
       " (0.750648883691072, 'elizabeth'),\n",
       " (0.7314496957870618, 'father'),\n",
       " (0.7296158352285126, 'kingdom'),\n",
       " (0.7280010324674799, 'mother')]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(value,key) for (key,value) in cosineDict.items()], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.8092078966752569, 'priest'),\n",
       " (0.804460582009797, 'church'),\n",
       " (0.7725253836901848, 'orthodox'),\n",
       " (0.7700323232560121, 'catholic'),\n",
       " (0.7411272859589869, 'congregation'),\n",
       " (0.7331976241792784, 'pastor'),\n",
       " (0.7090660804573766, 'christian'),\n",
       " (0.6940143018853606, 'clergyman'),\n",
       " (0.6935899138284772, 'roman'),\n",
       " (0.6829545665503904, 'faith')]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = word2vec['teacher'] - word2vec['school'] + word2vec['church']\n",
    "cosineDict = {w:cosDist(v,word2vec[w]) for w in word2vec}\n",
    "sorted([(value,key) for (key,value) in cosineDict.items()], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
