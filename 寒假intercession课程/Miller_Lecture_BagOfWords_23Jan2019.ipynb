{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```raw``` method returns a string of raw text from an NLTK corpus.  Let's get the raw text for the first article in the Reuters corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASIAN EXPORTERS FEAR DAMAGE FROM U.S.-JAPAN RIFT\n",
      "  Mounting trade friction between the\n",
      "  U.S. And Japan has raised fears among many of Asia's exporting\n",
      "  nations that the row could inflict far-reaching economic\n",
      "  damage, businessmen and officials said.\n",
      "      They told Reuter correspondents in Asian capitals a U.S.\n",
      "  Move against Japan might boost protectionist sentiment in the\n",
      "  U.S. And lead to curbs on American imports of their products.\n",
      "      But some exporters said that while the conflict would hurt\n",
      "  them in the long-run, in the short-term Tokyo's loss might be\n",
      "  their gain.\n",
      "      The U.S. Has said it will impose 300 mln dlrs of tariffs on\n",
      "  imports of Japanese electronics goods on April 17, in\n",
      "  retaliation for Japan's alleged failure to stick to a pact not\n",
      "  to sell semiconductors on world markets at below cost.\n",
      "      Unofficial Japanese estimates put the impact of the tariffs\n",
      "  at 10 billion dlrs and spokesmen for major electronics firms\n",
      "  said they would virtually halt exports of products hit by the\n",
      "  new taxes.\n",
      "      \"We wouldn't be able to do business,\" said a spokesman for\n",
      "  leading Japanese electronics firm Matsushita Electric\n",
      "  Industrial Co Ltd &lt;MC.T>.\n",
      "      \"If the tariffs remain in place for any length of time\n",
      "  beyond a few months it will mean the complete erosion of\n",
      "  exports (of goods subject to tariffs) to the U.S.,\" said Tom\n",
      "  Murtha, a stock analyst at the Tokyo office of broker &lt;James\n",
      "  Capel and Co>.\n",
      "      In Taiwan, businessmen and officials are also worried.\n",
      "      \"We are aware of the seriousness of the U.S. Threat against\n",
      "  Japan because it serves as a warning to us,\" said a senior\n",
      "  Taiwanese trade official who asked not to be named.\n",
      "      Taiwan had a trade trade surplus of 15.6 billion dlrs last\n",
      "  year, 95 pct of it with the U.S.\n",
      "      The surplus helped swell Taiwan's foreign exchange reserves\n",
      "  to 53 billion dlrs, among the world's largest.\n",
      "      \"We must quickly open our markets, remove trade barriers and\n",
      "  cut import tariffs to allow imports of U.S. Products, if we\n",
      "  want to defuse problems from possible U.S. Retaliation,\" said\n",
      "  Paul Sheen, chairman of textile exporters &lt;Taiwan Safe Group>.\n",
      "      A senior official of South Korea's trade promotion\n",
      "  association said the trade dispute between the U.S. And Japan\n",
      "  might also lead to pressure on South Korea, whose chief exports\n",
      "  are similar to those of Japan.\n",
      "      Last year South Korea had a trade surplus of 7.1 billion\n",
      "  dlrs with the U.S., Up from 4.9 billion dlrs in 1985.\n",
      "      In Malaysia, trade officers and businessmen said tough\n",
      "  curbs against Japan might allow hard-hit producers of\n",
      "  semiconductors in third countries to expand their sales to the\n",
      "  U.S.\n",
      "      In Hong Kong, where newspapers have alleged Japan has been\n",
      "  selling below-cost semiconductors, some electronics\n",
      "  manufacturers share that view. But other businessmen said such\n",
      "  a short-term commercial advantage would be outweighed by\n",
      "  further U.S. Pressure to block imports.\n",
      "      \"That is a very short-term view,\" said Lawrence Mills,\n",
      "  director-general of the Federation of Hong Kong Industry.\n",
      "      \"If the whole purpose is to prevent imports, one day it will\n",
      "  be extended to other sources. Much more serious for Hong Kong\n",
      "  is the disadvantage of action restraining trade,\" he said.\n",
      "      The U.S. Last year was Hong Kong's biggest export market,\n",
      "  accounting for over 30 pct of domestically produced exports.\n",
      "      The Australian government is awaiting the outcome of trade\n",
      "  talks between the U.S. And Japan with interest and concern,\n",
      "  Industry Minister John Button said in Canberra last Friday.\n",
      "      \"This kind of deterioration in trade relations between two\n",
      "  countries which are major trading partners of ours is a very\n",
      "  serious matter,\" Button said.\n",
      "      He said Australia's concerns centred on coal and beef,\n",
      "  Australia's two largest exports to Japan and also significant\n",
      "  U.S. Exports to that country.\n",
      "      Meanwhile U.S.-Japanese diplomatic manoeuvres to solve the\n",
      "  trade stand-off continue.\n",
      "      Japan's ruling Liberal Democratic Party yesterday outlined\n",
      "  a package of economic measures to boost the Japanese economy.\n",
      "      The measures proposed include a large supplementary budget\n",
      "  and record public works spending in the first half of the\n",
      "  financial year.\n",
      "      They also call for stepped-up spending as an emergency\n",
      "  measure to stimulate the economy despite Prime Minister\n",
      "  Yasuhiro Nakasone's avowed fiscal reform program.\n",
      "      Deputy U.S. Trade Representative Michael Smith and Makoto\n",
      "  Kuroda, Japan's deputy minister of International Trade and\n",
      "  Industry (MITI), are due to meet in Washington this week in an\n",
      "  effort to end the dispute.\n",
      "  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rawtext = reuters.raw('test/14826')\n",
    "print(rawtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```words``` method does tokenization for an NLTK corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "899"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_simplistic = reuters.words('test/14826')\n",
    "len(words_simplistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, ```words``` uses ```WordPunctTokenizer``` is a simplistic tokenizer based on punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U', '.', 'S', '.-', 'JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'between', 'the', 'U', '.', 'S', '.', 'And', 'Japan', 'has', 'raised', 'fears', 'among', 'many', 'of', 'Asia', \"'\", 's', 'exporting', 'nations', 'that', 'the', 'row', 'could', 'inflict', 'far', '-', 'reaching', 'economic', 'damage', ',', 'businessmen', 'and', 'officials', 'said', '.', 'They', 'told', 'Reuter', 'correspondents', 'in', 'Asian', 'capitals', 'a', 'U', '.', 'S', '.', 'Move', 'against', 'Japan', 'might', 'boost', 'protectionist', 'sentiment', 'in', 'the', 'U', '.', 'S', '.', 'And', 'lead', 'to', 'curbs', 'on', 'American', 'imports', 'of', 'their', 'products', '.', 'But', 'some', 'exporters', 'said', 'that', 'while', 'the', 'conflict', 'would', 'hurt', 'them', 'in', 'the', 'long', '-']\n"
     ]
    }
   ],
   "source": [
    "print(words_simplistic[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is preferable use the ```word_tokenize``` method which uses the more sophisticated ```TreebankTokenizer```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "816"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = nltk.word_tokenize(rawtext)\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ASIAN', 'EXPORTERS', 'FEAR', 'DAMAGE', 'FROM', 'U.S.-JAPAN', 'RIFT', 'Mounting', 'trade', 'friction', 'between', 'the', 'U.S.', 'And', 'Japan', 'has', 'raised', 'fears', 'among', 'many', 'of', 'Asia', \"'s\", 'exporting', 'nations', 'that', 'the', 'row', 'could', 'inflict', 'far-reaching', 'economic', 'damage', ',', 'businessmen', 'and', 'officials', 'said', '.', 'They', 'told', 'Reuter', 'correspondents', 'in', 'Asian', 'capitals', 'a', 'U.S.', 'Move', 'against', 'Japan', 'might', 'boost', 'protectionist', 'sentiment', 'in', 'the', 'U.S.', 'And', 'lead', 'to', 'curbs', 'on', 'American', 'imports', 'of', 'their', 'products', '.', 'But', 'some', 'exporters', 'said', 'that', 'while', 'the', 'conflict', 'would', 'hurt', 'them', 'in', 'the', 'long-run', ',', 'in', 'the', 'short-term', 'Tokyo', \"'s\", 'loss', 'might', 'be', 'their', 'gain', '.', 'The', 'U.S.', 'Has', 'said', 'it']\n"
     ]
    }
   ],
   "source": [
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build a dictionary of words to record the count of each word that is used in this article.  Python dictionaries are (behind the scenes) implemented using hash tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordFreqDict = {}\n",
    "type(wordFreqDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in words:\n",
    "    if word in wordFreqDict:\n",
    "        wordFreqDict[word] += 1\n",
    "    else:\n",
    "        wordFreqDict[word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordFreqDict['Japan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "387"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(wordFreqDict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A collection of objects and their associated counts is called a ***multiset*** or a ***bag***.  When applied to the count of words in a document, this is called the ***bag of words*** model for a text document.  The bag of words does not see the order that the words are used, only their frequencies.  NLP tasks such as document retrieval and classification are commonly implemented using a bag of words model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we try to query a term that was not in the article?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'volcano'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-fdca403d0965>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwordFreqDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'volcano'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'volcano'"
     ]
    }
   ],
   "source": [
    "wordFreqDict['volcano']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a subclass of the ```dict``` type called ```Counter``` that more naturally handles multisets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "issubclass(Counter,dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordFreqCounter = Counter(words)\n",
    "wordFreqCounter['Japan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordFreqCounter['volcano']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might recall that NLTK had its own function called ```FreqDist``` that also implements term counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.probability.FreqDist"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordFreqNLTK = nltk.FreqDist(words)\n",
    "type(wordFreqNLTK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact the ```nltk.probability.FreqDist``` type is a subclass of ```Counter```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issubclass(nltk.probability.FreqDist, Counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'the': 32, '.': 31, 'of': 30, ',': 29, 'to': 26, 'said': 16, 'a': 14, 'trade': 13, 'U.S.': 13, 'in': 13, ...})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordFreqNLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scikit-learn** has its own tools for extracting word frquencies from a corpus of texts. The tools are powerful but take some getting used to.<br>\n",
    "\n",
    "```CountVectorizer``` acts on a corpus (list of raw text documents), does its own tokenization, and then returns matrix of word counts.  Each row represents a document and each column represents a word in the vocabulary of the corpus.<br>\n",
    "\n",
    "However, since such a matrix is usually quite sparse, the matrix gets represented as a ***sparse matrix*** by storing a list of only the nonzero entries.<br>\n",
    "\n",
    "In the following example, we'll just pass a list of only one document to ```CountVectorizer```, so we'll get back a sparse matrix with only one row.<br>\n",
    "\n",
    "The below link is a good reference:<br>\n",
    "https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x351 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 351 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "wordSKcount = count_vect.fit_transform([rawtext])\n",
    "wordSKcount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 97)\t1\n",
      "  (0, 93)\t1\n",
      "  (0, 334)\t1\n",
      "  (0, 332)\t1\n",
      "  (0, 189)\t1\n",
      "  (0, 90)\t1\n",
      "  (0, 194)\t1\n",
      "  (0, 151)\t1\n",
      "  (0, 161)\t1\n",
      "  (0, 175)\t1\n",
      "  (0, 280)\t1\n",
      "  (0, 190)\t1\n",
      "  (0, 255)\t1\n",
      "  (0, 80)\t2\n",
      "  (0, 240)\t1\n",
      "  (0, 251)\t1\n",
      "  (0, 117)\t1\n",
      "  (0, 32)\t1\n",
      "  (0, 203)\t1\n",
      "  (0, 348)\t1\n",
      "  (0, 235)\t1\n",
      "  (0, 81)\t1\n",
      "  (0, 291)\t1\n",
      "  (0, 187)\t1\n",
      "  (0, 96)\t1\n",
      "  :\t:\n",
      "  (0, 71)\t1\n",
      "  (0, 261)\t1\n",
      "  (0, 305)\t5\n",
      "  (0, 205)\t1\n",
      "  (0, 104)\t1\n",
      "  (0, 25)\t1\n",
      "  (0, 209)\t30\n",
      "  (0, 179)\t1\n",
      "  (0, 17)\t2\n",
      "  (0, 110)\t1\n",
      "  (0, 248)\t1\n",
      "  (0, 133)\t3\n",
      "  (0, 20)\t16\n",
      "  (0, 306)\t37\n",
      "  (0, 41)\t4\n",
      "  (0, 120)\t1\n",
      "  (0, 320)\t15\n",
      "  (0, 198)\t1\n",
      "  (0, 260)\t1\n",
      "  (0, 155)\t13\n",
      "  (0, 122)\t3\n",
      "  (0, 76)\t2\n",
      "  (0, 109)\t1\n",
      "  (0, 103)\t3\n",
      "  (0, 26)\t2\n"
     ]
    }
   ],
   "source": [
    "print(wordSKcount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last entry means that in row 0 (representing the first document) and column 26 (representing one of the words in the vocabulary) has a count of 2.  What word does column 26 represent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'asian'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect.get_feature_names()[26]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And how do we go from the vocabulary word to its column index?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "print(count_vect.vocabulary_.get('asian'))  # Indexed by lower-case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(count_vect.vocabulary_.get('u.s.')) # 'u.s.' is missing from the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print the entire dictionary of vocabulary words and their column indices.  (These are not word counts!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'asian': 26, 'exporters': 103, 'fear': 109, 'damage': 76, 'from': 122, 'japan': 155, 'rift': 260, 'mounting': 198, 'trade': 320, 'friction': 120, 'between': 41, 'the': 306, 'and': 20, 'has': 133, 'raised': 248, 'fears': 110, 'among': 17, 'many': 179, 'of': 209, 'asia': 25, 'exporting': 104, 'nations': 205, 'that': 305, 'row': 261, 'could': 71, 'inflict': 149, 'far': 108, 'reaching': 249, 'economic': 91, 'businessmen': 50, 'officials': 214, 'said': 265, 'they': 309, 'told': 317, 'reuter': 259, 'correspondents': 69, 'in': 145, 'capitals': 57, 'move': 199, 'against': 12, 'might': 191, 'boost': 46, 'protectionist': 243, 'sentiment': 271, 'lead': 166, 'to': 315, 'curbs': 74, 'on': 215, 'american': 16, 'imports': 143, 'their': 307, 'products': 239, 'but': 51, 'some': 282, 'while': 337, 'conflict': 67, 'would': 346, 'hurt': 139, 'them': 308, 'long': 170, 'run': 263, 'short': 277, 'term': 303, 'tokyo': 316, 'loss': 171, 'be': 36, 'gain': 124, 'it': 153, 'will': 341, 'impose': 144, '300': 5, 'mln': 195, 'dlrs': 87, 'tariffs': 301, 'japanese': 156, 'electronics': 95, 'goods': 126, 'april': 22, '17': 2, 'retaliation': 258, 'for': 118, 'alleged': 13, 'failure': 107, 'stick': 290, 'pact': 226, 'not': 208, 'sell': 267, 'semiconductors': 269, 'world': 344, 'markets': 181, 'at': 29, 'below': 40, 'cost': 70, 'unofficial': 323, 'estimates': 99, 'put': 246, 'impact': 141, '10': 0, 'billion': 44, 'spokesmen': 287, 'major': 174, 'firms': 115, 'virtually': 328, 'halt': 131, 'exports': 105, 'hit': 137, 'by': 53, 'new': 206, 'taxes': 302, 'we': 333, 'wouldn': 347, 'able': 8, 'do': 88, 'business': 49, 'spokesman': 286, 'leading': 167, 'firm': 114, 'matsushita': 182, 'electric': 94, 'industrial': 147, 'co': 61, 'ltd': 173, 'lt': 172, 'mc': 184, 'if': 140, 'remain': 253, 'place': 231, 'any': 21, 'length': 168, 'time': 314, 'beyond': 42, 'few': 112, 'months': 196, 'mean': 185, 'complete': 64, 'erosion': 98, 'subject': 293, 'tom': 318, 'murtha': 201, 'stock': 292, 'analyst': 19, 'office': 211, 'broker': 47, 'james': 154, 'capel': 56, 'taiwan': 298, 'are': 23, 'also': 15, 'worried': 345, 'aware': 34, 'seriousness': 273, 'threat': 313, 'because': 37, 'serves': 274, 'as': 24, 'warning': 330, 'us': 325, 'senior': 270, 'taiwanese': 299, 'official': 213, 'who': 338, 'asked': 27, 'named': 204, 'had': 129, 'surplus': 296, '15': 1, 'last': 164, 'year': 349, '95': 7, 'pct': 230, 'with': 342, 'helped': 136, 'swell': 297, 'foreign': 119, 'exchange': 100, 'reserves': 256, '53': 6, 'largest': 163, 'must': 202, 'quickly': 247, 'open': 217, 'our': 219, 'remove': 254, 'barriers': 35, 'cut': 75, 'import': 142, 'allow': 14, 'want': 329, 'defuse': 78, 'problems': 236, 'possible': 232, 'paul': 229, 'sheen': 276, 'chairman': 59, 'textile': 304, 'safe': 264, 'group': 128, 'south': 284, 'korea': 160, 'promotion': 241, 'association': 28, 'dispute': 86, 'pressure': 233, 'whose': 340, 'chief': 60, 'similar': 279, 'those': 312, 'up': 324, '1985': 3, 'malaysia': 176, 'officers': 212, 'tough': 319, 'hard': 132, 'producers': 238, 'third': 310, 'countries': 72, 'expand': 101, 'sales': 266, 'hong': 138, 'kong': 159, 'where': 335, 'newspapers': 207, 'have': 134, 'been': 39, 'selling': 268, 'manufacturers': 178, 'share': 275, 'view': 327, 'other': 218, 'such': 294, 'commercial': 63, 'advantage': 11, 'outweighed': 223, 'further': 123, 'block': 45, 'is': 152, 'very': 326, 'lawrence': 165, 'mills': 192, 'director': 84, 'general': 125, 'federation': 111, 'industry': 148, 'whole': 339, 'purpose': 245, 'prevent': 234, 'one': 216, 'day': 77, 'extended': 106, 'sources': 283, 'much': 200, 'more': 197, 'serious': 272, 'disadvantage': 85, 'action': 10, 'restraining': 257, 'he': 135, 'was': 331, 'biggest': 43, 'export': 102, 'market': 180, 'accounting': 9, 'over': 224, '30': 4, 'domestically': 89, 'produced': 237, 'australian': 31, 'government': 127, 'awaiting': 33, 'outcome': 221, 'talks': 300, 'interest': 150, 'concern': 65, 'minister': 193, 'john': 157, 'button': 52, 'canberra': 55, 'friday': 121, 'this': 311, 'kind': 158, 'deterioration': 82, 'relations': 252, 'two': 322, 'which': 336, 'trading': 321, 'partners': 227, 'ours': 220, 'matter': 183, 'australia': 30, 'concerns': 66, 'centred': 58, 'coal': 62, 'beef': 38, 'significant': 278, 'country': 73, 'meanwhile': 186, 'diplomatic': 83, 'manoeuvres': 177, 'solve': 281, 'stand': 288, 'off': 210, 'continue': 68, 'ruling': 262, 'liberal': 169, 'democratic': 79, 'party': 228, 'yesterday': 350, 'outlined': 222, 'package': 225, 'measures': 188, 'economy': 92, 'proposed': 242, 'include': 146, 'large': 162, 'supplementary': 295, 'budget': 48, 'record': 250, 'public': 244, 'works': 343, 'spending': 285, 'first': 116, 'half': 130, 'financial': 113, 'call': 54, 'stepped': 289, 'an': 18, 'emergency': 96, 'measure': 187, 'stimulate': 291, 'despite': 81, 'prime': 235, 'yasuhiro': 348, 'nakasone': 203, 'avowed': 32, 'fiscal': 117, 'reform': 251, 'program': 240, 'deputy': 80, 'representative': 255, 'michael': 190, 'smith': 280, 'makoto': 175, 'kuroda': 161, 'international': 151, 'miti': 194, 'due': 90, 'meet': 189, 'washington': 332, 'week': 334, 'effort': 93, 'end': 97}\n"
     ]
    }
   ],
   "source": [
    "print(count_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Scikit-learn's ```CountVectorizer``` to process an entire list of documents.  Let's first build a list of raw text documents from the Reuters corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10788"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rawtextList = [reuters.raw(id) for id in reuters.fileids()]\n",
    "len(rawtextList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the first article was about U.S.-Japan trade frictions.  What is the next article about?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHINA DAILY SAYS VERMIN EAT 7-12 PCT GRAIN STOCKS\n",
      "  A survey of 19 provinces and seven cities\n",
      "  showed vermin consume between seven and 12 pct of China's grain\n",
      "  stocks, the China Daily said.\n",
      "      It also said that each year 1.575 mln tonnes, or 25 pct, of\n",
      "  China's fruit output are left to rot, and 2.1 mln tonnes, or up\n",
      "  to 30 pct, of its vegetables. The paper blamed the waste on\n",
      "  inadequate storage and bad preservation methods.\n",
      "      It said the government had launched a national programme to\n",
      "  reduce waste, calling for improved technology in storage and\n",
      "  preservation, and greater production of additives. The paper\n",
      "  gave no further details.\n",
      "  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(rawtextList[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply ```CountVectorizer``` to the entire corpus of 10,788 documents (articles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10788x30916 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 785208 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_vect = CountVectorizer()\n",
    "countMatrix = count_vect.fit_transform(rawtextList)\n",
    "countMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 10,788 rows corresponding to each document, and 30,916 columns corresponding to a vocabulary of size 30,916 words or tokens, and a total of 785,208 nonzero entries in the frequency matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's import Scikit-learn's function for cosine distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find the cosine distance between the first article on U.S.-Japan trade relations and the second article on rats eating Chinese grain and rotting fruit.  A cosine distance of **1.0** would be as close as possible, i.e. the word count vectors would point in the same direction (even if of different magnitude), i.e. the articles would have the same relative frequency of words.  A cosine distance of **0.0** would mean that the word count vectors are orthogonal, i.e. the articles would have no overlapping terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.56441791]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(countMatrix[0],countMatrix[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make a list of all cosine distances to the first article on U.S.-Japan trade relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.56441791, 0.68430926, ..., 0.01616928, 0.01901003,\n",
       "        0.0155811 ]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity(countMatrix[0],countMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We extract the first row of the array and convert it to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosList = cosine_similarity(countMatrix[0],countMatrix)[0].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```enumerate``` is a useful object for iterating over lists and their indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 'apple'), (1, 'banana'), (2, 'orange')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L = [\"apple\",\"banana\",\"orange\"]\n",
    "list(enumerate(L))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's reverse sort by cosine distance and also print out the indices of the corresponding documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0000000000000018, 0),\n",
       " (0.8441797447249986, 3605),\n",
       " (0.8255584044323317, 3311),\n",
       " (0.8226145598212536, 3781),\n",
       " (0.8225794792829114, 8967),\n",
       " (0.8189882935052504, 3244),\n",
       " (0.8179586571941766, 3464)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(((e,i) for i,e in enumerate(cosList)), reverse=True)[:7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the article that is closest in cosine distance to the first article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECONOMIC SPOTLIGHT - U.S. CONGRESS RAPS JAPAN\n",
      "  The U.S. Congress is making Japan,\n",
      "  with its enormous worldwide trade surplus, the symbol of the\n",
      "  U.S. trade crisis and the focus of its efforts to turn around\n",
      "  America's record trade deficit.\n",
      "      \"Japan has come to symbolize what we fear most in trade: the\n",
      "  challenge to our high technology industries, the threat of\n",
      "  government nutured competition, and the multitude of barriers\n",
      "  to our exports,\" Senate Democratic Leader Robert Byrd said.\n",
      "      \"If we can find a way to come to terms with Japan over trade\n",
      "  problems, we can manage our difficulties with other countries,\"\n",
      "  the West Virginia Democrat said at a Senate Finance Committee\n",
      "  hearing on the trade bill.\n",
      "      Byrd and House Speaker Jim Wright, a Texas Democrat, have\n",
      "  made trade legislation a priority this year and a wide-ranging\n",
      "  bill is being readied for probable House approval next month.\n",
      "      Japan's bilateral trade surplus jumped from 12 billion dlrs\n",
      "  in 1980 to 62 billion dlrs last year. Its surplus rose to 8.14\n",
      "  billion dlrs in February from 5.7 billion dlrs in January.\n",
      "      Congress points to the record 169 billion dlrs U.S. trade\n",
      "  deficit in 1986 and the slow response in the trade imbalance to\n",
      "  the dollar's decline in world currency markets as a reason to\n",
      "  press Japan to buy more U.S. goods.\n",
      "      They are particularly dismayed by the rapid deterioration\n",
      "  in U.S. exports of sophisticated computer technology.\n",
      "      In response to the growing anger and pressure by the U.S.\n",
      "  semiconductor industry, President Reagan Friday announced he\n",
      "  intended to raise tariffs as much as 300 mln dlrs on Japanese\n",
      "  electronic goods in retaliation for Japan's failure to abide by\n",
      "  a 1986 U.S.-Japanese semiconductor agreement.\n",
      "      Congress also has been been angered by the administration's\n",
      "  lack of success with Japan on a host of other trade issues\n",
      "  including beef, citrus, automobile parts, telecommunications\n",
      "  goods, and financial services.\n",
      "      The bulk of the House trade bill was written last week in\n",
      "  four committees. It is a package of trade sanctions and\n",
      "  measures to force the administration take tough action against\n",
      "  foreign trade barriers and unfair competition.\n",
      "      Although most provisions do not single out Japan, in many\n",
      "  cases their impact would be to restrict imports of Japanese\n",
      "  products or make them more expensive with higher duties.\n",
      "      The cornerstone of the trade legislation passed the House\n",
      "  Ways and Means Committee by a vote of 34 to 2. Its focus is to\n",
      "  force President Reagan to retaliate against unfair foreign\n",
      "  competition and to make it easier for U.S. industries to win\n",
      "  temporary relief from surges in imports.\n",
      "      The most controversial issue, an amendment to restrict\n",
      "  imports if countries such as Japan with large surpluses do not\n",
      "  buy more U.S. goods was left for an April vote by the House.\n",
      "      Rep. Richard Gephardt, a Democratic presidential aspirant\n",
      "  from Missouri, has the support of Wright and other key\n",
      "  Democrats to press for passage of the amendment.\n",
      "      The measure would have the most impact on Japan, West\n",
      "  Germany, Taiwan and South Korea. If Japan, for example, does\n",
      "  not reduce its barriers by mid-1988, the United States would\n",
      "  set import quotas or tariffs to cut Japanese surplus by ten per\n",
      "  cent a year for three years.\n",
      "      \"I'm tired of going into companies and having managers say\n",
      "  to me, 'We're not over competing in Japan because we can't\n",
      "  compete in the marketplace.' That argument needs to be taken\n",
      "  away from American business,\" Gephardt said.\n",
      "      The administration has said it could not support a trade\n",
      "  bill containing such a provision.\n",
      "      \n",
      "  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(rawtextList[3605])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat this exercise for the second article concerning Chinese grain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0, 1),\n",
       " (0.6550537786773223, 8111),\n",
       " (0.6188024107937812, 10312),\n",
       " (0.6148202576210995, 6256),\n",
       " (0.6130022570999106, 4162),\n",
       " (0.6092796480128674, 1441),\n",
       " (0.6068330093175274, 219)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosList2 = cosine_similarity(countMatrix[1],countMatrix)[0].tolist()\n",
    "sorted(((e,i) for i,e in enumerate(cosList2)), reverse=True)[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHINA TO IMPORT MORE GRAIN IN 1987\n",
      "  China's grain imports will rise\n",
      "  in 1987 because of a serious drought and increasing demand, but\n",
      "  will be not be as large as in the past, Chinese officials and\n",
      "  Japanese traders told Reuters.\n",
      "      They said foreign exchange constraints and national policy\n",
      "  would not allow a return to large-scale imports, which peaked\n",
      "  at 16.15 mln tonnes in 1982.\n",
      "      An agricultural official of the Shanghai government put\n",
      "  maximum imports at about 10 mln tonnes this year, against 7.73\n",
      "  mln in 1986 and 5.97 mln in 1985.\n",
      "      Officials said grain imports rose in 1986 because of a poor\n",
      "  harvest and rising domestic demand, but remained below exports,\n",
      "  which rose to 9.42 mln tonnes from 9.33 mln in 1985.\n",
      "      \"China is short of foreign exchange,\" the Shanghai official\n",
      "  said. \"We cannot rely on imports, even at current low world\n",
      "  prices. Only if there is a major disaster will we become a\n",
      "  major importer.\"\n",
      "      A Japanese trader in Peking said Chinese grain imports\n",
      "  would rise and exports fall this year because of the drought,\n",
      "  low world prices and rising domestic demand for human and\n",
      "  animal consumption.\n",
      "      \"At current prices, China loses yuan on every tonne of grain\n",
      "  it exports, though it earns foreign exchange which it badly\n",
      "  needs,\" the trader said.\n",
      "      The People's Daily said last Saturday a serious drought is\n",
      "  affecting 13.3 mln hectares of arable land, which will reduce\n",
      "  the summer grain harvest from last year's level.\n",
      "      The paper added that leaders in some areas were not paying\n",
      "  enough attention to agriculture, especially grain, making it\n",
      "  difficult to achieve the 1987 grain output target of 405 mln\n",
      "  tonnes against 391 mln in 1986.\n",
      "      \"All areas must spare no effort to raise the autumn harvest\n",
      "  area, especially of corn, sweet potatoes, paddy rice and\n",
      "  high-yield cash crops,\" it said.\n",
      "      It added factory production might have to be reduced to\n",
      "  provide electricity for agriculture if it was needed to fight\n",
      "  the drought.\n",
      "      Since January, the press has devoted much attention to\n",
      "  grain, stressing that growth in output is vital to China's\n",
      "  economic and political stability and that prices paid to\n",
      "  farmers are too low.\n",
      "      Officials in east China have repeatedly said stable grain\n",
      "  production is a key state policy and outlined the measures\n",
      "  being taken in their areas to encourage output.\n",
      "      The Shanghai official said that in one suburb, 10 pct of\n",
      "  the pre-tax profits of factories are used to subsidise\n",
      "  agriculture. He said rural industries in other suburbs also set\n",
      "  aside money for grain and pay the salaries of some of the\n",
      "  70,000 workers available to help farmers.\n",
      "      Chu Jinfeng, an official of Fengbing county outside\n",
      "  Shanghai, said factory workers get 60 yuan a month and three\n",
      "  years unpaid leave to grow grain and can keep the profits.\n",
      "      Pan Huashan, an official of the agricultural department of\n",
      "  Zhejiang Province, said rural industry also subsidises grain\n",
      "  output in his province.\n",
      "      \"In addition, we are setting up grain production bases,\n",
      "  raising the level of science and technology on the farms and\n",
      "  improving the supply of raw materials, roads and other\n",
      "  infrastructure,\" he said.\n",
      "      The Shanghai official said rural residents who work in\n",
      "  industry or commerce usually keep their land to farm in their\n",
      "  spare time, or let other family members farm it. In some cases,\n",
      "  they lease the land to grain farmers.\n",
      "      The China Daily said last month that grain output should\n",
      "  reach between 425 and 450 mln tonnes by 1990 and between 480\n",
      "  and 500 mln by 2000. It said growing grain should be made\n",
      "  profitable.\n",
      "      \"The advantages the state promises grain growers actually\n",
      "  yield tangible profits for them and are not siphoned off by\n",
      "  intermediate agencies because of bureaucracy or corruption.\n",
      "  Only this will boost enthusiasm,\" it said.\n",
      "  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(rawtextList[8111])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One disadvantage of using word counts for assessing the similarity of documents is that it tends to overemphasize very frequent words like \"the\" and \"of\", and overemphasize less common words that may have more *semantic* (i.e. meaningful) content.  We can do this by dividing by a weighting that is related to the frequency of the word, effectively reducing the weight of the most frequent terms.  The resulting statistic is called the ***term frequency - inverse document frequency***, or ***tf-idf***.<br>\n",
    "\n",
    "Let $\\operatorname{tf}(t,d)$ be the raw count of term $t$ in document $d$, and let $\\operatorname{df}(t)$ be the number of documents that contain term $t$.  Then a typical choice (and ```sklearn```'s default) is to let the inverse docuemnt frequency be<br>\n",
    "$$\\operatorname{idf}(t) = \\log \\frac{N_d + 1}{\\operatorname{df}(t) + 1} + 1$$\n",
    "where $N_d$ is the total number of documents.  The $+1$'s are used for smoothing and prevent division by zero and other nastiness.\n",
    "\n",
    "We can use ```TfidfVectorizer``` to create a feature matrix of *tf-idf* statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<10788x30916 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 785208 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "tfidfMatrix = tfidf_vect.fit_transform(rawtextList)\n",
    "tfidfMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3384\n",
      "27902\n",
      "3384\n",
      "27902\n"
     ]
    }
   ],
   "source": [
    "print(count_vect.vocabulary_.get('asian'))\n",
    "print(count_vect.vocabulary_.get('the'))\n",
    "print(tfidf_vect.vocabulary_.get('asian'))\n",
    "print(tfidf_vect.vocabulary_.get('the'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first article, the term *the* appears 18.5 times more often than the term *asian*.  But after applying the inverse document frequency weighting, the tf-idf statistic for *the* is only 4.2 times the statistic for *asian*, because *the* is such a common word throughout the entire corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.5\n",
      "4.203716670286382\n"
     ]
    }
   ],
   "source": [
    "print(countMatrix[0,27902]/countMatrix[0,3384])\n",
    "print(tfidfMatrix[0,27902]/tfidfMatrix[0,3384])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compare the similarity ranking to the first article using raw term counts versus the tf-idf statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1.0000000000000018, 0),\n",
       " (0.8441797447249986, 3605),\n",
       " (0.8255584044323317, 3311),\n",
       " (0.8226145598212536, 3781),\n",
       " (0.8225794792829114, 8967),\n",
       " (0.8189882935052504, 3244),\n",
       " (0.8179586571941766, 3464)]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(((e,i) for i,e in enumerate(cosList)), reverse=True)[:7]  # This uses raw term counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.9999999999999998, 0),\n",
       " (0.5413540058297731, 3464),\n",
       " (0.5310550431375045, 3605),\n",
       " (0.5048206799420962, 8967),\n",
       " (0.4960254505076737, 3781),\n",
       " (0.49107228481411275, 1118),\n",
       " (0.4845059876298639, 4635)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosList_tfidf = cosine_similarity(tfidfMatrix[0],tfidfMatrix)[0].tolist()\n",
    "sorted(((e,i) for i,e in enumerate(cosList_tfidf)), reverse=True)[:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
