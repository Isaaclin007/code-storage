{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work our way through the example on this page:\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/\n",
    "\n",
    "Some of this work will involve understanding aspects of Python.\n",
    "\n",
    "The latent Dirichlet allocation model was introduced first as a technique in genetics, and original idea of using this technique in topic analysis comes from this paper:\n",
    "\n",
    "Blei, David M.; Ng, Andrew Y.; Jordan, Michael I (January 2003). Lafferty, John, ed. \"Latent Dirichlet Allocation\". Journal of Machine Learning Research. 3 (4–5): pp. 993–1022. doi:10.1162/jmlr.2003.3.4-5.993.\n",
    "\n",
    "There is also a Wikipedia article:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\n",
    "\n",
    "This is a nice friendly and readable paper:\n",
    "\n",
    "http://obphio.us/pdfs/lda_tutorial.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by preparing a sample list of documents. Here we start with a documents that are simply text strings and eventually we extract for each document a list of words by various possible means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\"\n",
    "doc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\n",
    "doc3 = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\n",
    "doc4 = \"Sometimes I feel pressure to perform well at school, but my father never seems to drive my sister to do better.\"\n",
    "doc5 = \"Health experts say that Sugar is not good for your lifestyle.\"\n",
    "\n",
    "# compile documents\n",
    "doc_complete = [doc1, doc2, doc3, doc4, doc5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Document Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to \"clean\" the documents. Which involves the following steps:\n",
    "\n",
    "1) convert all characters to lower case\n",
    "2) remove common words that convey less meaning (at least in terms of word frequencies)\n",
    "3) remove puncuation marks\n",
    "4) lemmatize each word - e.g. reduce \"talks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Python, a string can be reduced to lower case using the .lower() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'if you work  hard, you will be  rewarded, according to mary.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydoc=\"If you work  hard, you will be  rewarded, according to Mary.\"\n",
    "mydoc.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful Python tool device is that we can split a string on a delimiter. By default, the delimiter is the space character. Note that if the delimiter appears multiple times in the same chunk, we don't get a empty strings in the resulting list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['If you work  hard', ' you will be  rewarded', ' according to Mary.']\n",
      "['If', 'you', 'work', 'hard,', 'you', 'will', 'be', 'rewarded,', 'according', 'to', 'Mary.']\n"
     ]
    }
   ],
   "source": [
    "print(mydoc.split(\",\"))\n",
    "print(mydoc.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a Python function for joining the strings in a list and inserting any character as a separator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "today_will_be_a_great_day\n",
      "today,will,be,a,great,day\n",
      "today will be a great day\n"
     ]
    }
   ],
   "source": [
    "wordlist=[\"today\",\"will\",\"be\",\"a\",\"great\",\"day\"]\n",
    "print(\"_\".join(wordlist))\n",
    "print(\",\".join(wordlist))\n",
    "print(\" \".join(wordlist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also want to remove punctuation marks. The string package also provide us with a list of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "punct=string.punctuation\n",
    "print(punct)\n",
    "print(type(punct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the string.punctuation object is a string. \n",
    "We can convert this to a set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{':', '=', '#', '&', '\\\\', '`', '[', '<', '{', '?', ')', '\"', '+', '-', '*', '@', '^', ';', '(', '!', '_', ',', '/', '|', '.', '>', '%', '$', '}', '~', \"'\", ']'}\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "punct=set(string.punctuation)\n",
    "print(punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use list comprehension to remove all punctuation marks from a string. First, we can get all the characters in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 's', ' ', 't', 'h', 'i', 's', ' ', 'a', ' ', 't', 'e', 's', 't', '?', ' ', 'I', ' ', 't', 'h', 'o', 'u', 'g', 'h', 't', ' ', 'y', 'o', 'u', ' ', 'w', 'e', 'r', 'e', ' ', 't', 'e', 's', 't', 'i', 'n', 'g', '!', ' ', 'P', 'l', 'e', 'a', 's', 'e', ' ', 'w', 'a', 'r', 'n', ' ', 'm', 'e', ' ', 'n', 'e', 'x', 't', ' ', 't', 'i', 'm', 'e', '.']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "punct=set(string.punctuation)\n",
    "text=\"Is this a test? I thought you were testing! Please warn me next time.\"\n",
    "textlist=[ch for ch in text]\n",
    "print(textlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can put an extra condition in for inclusion of a character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 's', ' ', 't', 'h', 'i', 's', ' ', 'a', ' ', 't', 'e', 's', 't', ' ', 'I', ' ', 't', 'h', 'o', 'u', 'g', 'h', 't', ' ', 'y', 'o', 'u', ' ', 'w', 'e', 'r', 'e', ' ', 't', 'e', 's', 't', 'i', 'n', 'g', ' ', 'P', 'l', 'e', 'a', 's', 'e', ' ', 'w', 'a', 'r', 'n', ' ', 'm', 'e', ' ', 'n', 'e', 'x', 't', ' ', 't', 'i', 'm', 'e']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "punct=set(string.punctuation)\n",
    "text=\"Is this a test? I thought you were testing! Please warn me next time.\"\n",
    "textlist=[ch for ch in text if ch not in punct]\n",
    "print(textlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can join characters to get a single string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is this a test I thought you were testing Please warn me next time\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "punct=set(string.punctuation)\n",
    "text=\"Is this a test? I thought you were testing! Please warn me next time.\"\n",
    "newtext=\"\".join([ch for ch in text if ch not in punct])\n",
    "print(newtext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can write a punctuation remover in one line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Is this a test I thought you were testing Please warn me next time'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "punct=set(string.punctuation)\n",
    "def remove_punctuation(text):\n",
    "    newtext=\"\".join([ch for ch in text if ch not in punct])\n",
    "    return(newtext)\n",
    "text=\"Is this a test? I thought you were testing! Please warn me next time.\"\n",
    "remove_punctuation(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting all characters to lower case is an easy add-in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'is this a test i thought you were testing please warn me next time'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "punct=set(string.punctuation)\n",
    "def clean_doc(text):\n",
    "    newtext=\"\".join([ch.lower() for ch in text if ch not in punct])\n",
    "    return(newtext)\n",
    "text=\"Is this a test? I thought you were testing! Please warn me next time.\"\n",
    "clean_doc(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nltk package contains a list of stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The package provides a word lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "talk\n",
      "dog\n",
      "argues\n",
      "foot\n",
      "wish\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize(\"talks\"))\n",
    "print(lemma.lemmatize(\"dogs\"))\n",
    "print(lemma.lemmatize(\"argues\"))\n",
    "print(lemma.lemmatize(\"feet\"))\n",
    "print(lemma.lemmatize(\"wishes\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a function to lemmatize words in a piece of text and remove stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generally favor test try one\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "# creat a set of stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(text):\n",
    "    split_text=text.split()\n",
    "    lemmatized_text=[lemma.lemmatize(i) for i in split_text]\n",
    "    stops_removed=[i for i in lemmatized_text if i not in stop_words]\n",
    "    textnew=\" \".join(stops_removed)\n",
    "    return(textnew)\n",
    "\n",
    "text=\"i am generally not in favor of tests but i will try this one\"\n",
    "textnew=clean(text)\n",
    "print(textnew)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together, we create a single function that cleans a document.\n",
    "The final output is a list of lists of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fit', 'three', 'angel', 'head', 'pin', 'know', 'study', 'mathematics', 'know', 'mathematician', 'know', 'lot', 'counting']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punct=set(string.punctuation)\n",
    "\n",
    "def clean_doc(text):\n",
    "    newtext=\"\".join([ch.lower() for ch in text if ch not in punct])\n",
    "    split_text=newtext.split()\n",
    "    lemmatized_text=[lemma.lemmatize(i) for i in split_text]\n",
    "    stops_removed=[i for i in lemmatized_text if i not in stop_words]\n",
    "    return(stops_removed)\n",
    "\n",
    "text=\"You can fit more then three angels on the head of a pin. \\\n",
    "    I know this because I study mathematics.\\\n",
    "    As you know, mathematicians know a lot about counting.\"\n",
    "print(clean_doc(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['sugar', 'bad', 'consume', 'sister', 'like', 'sugar', 'father'], ['father', 'spends', 'lot', 'time', 'driving', 'sister', 'around', 'dance', 'practice'], ['doctor', 'suggest', 'driving', 'may', 'cause', 'increased', 'stress', 'blood', 'pressure'], ['sometimes', 'feel', 'pressure', 'perform', 'well', 'school', 'father', 'never', 'seems', 'drive', 'sister', 'better'], ['health', 'expert', 'say', 'sugar', 'good', 'lifestyle']]\n"
     ]
    }
   ],
   "source": [
    "doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\"\n",
    "doc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\n",
    "doc3 = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\n",
    "doc4 = \"Sometimes I feel pressure to perform well at school, but my father never seems to drive my sister to do better.\"\n",
    "doc5 = \"Health experts say that Sugar is not good for your lifestyle.\"\n",
    "\n",
    "# compile documents\n",
    "doc_complete = [doc1, doc2, doc3, doc4, doc5]\n",
    "docs_cleaned=[clean_doc(doc) for doc in doc_complete]\n",
    "print(docs_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Document-Term Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis carried out uses the word frequencies in every document.\n",
    "This information is stored in a document term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a \"dictionary\" from our corpus of documents this is nothing more than \n",
    "a translation between words in our entire corpus and numbers, with a unique \n",
    "number for every word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(docs_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dictionary has a list of words, and number codes for those words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "\n",
      "\n",
      "['bad', 'consume', 'father', 'like', 'sister', 'sugar', 'around', 'dance', 'driving', 'lot', 'practice', 'spends', 'time', 'blood', 'cause', 'doctor', 'increased', 'may', 'pressure', 'stress', 'suggest', 'better', 'drive', 'feel', 'never', 'perform', 'school', 'seems', 'sometimes', 'well', 'expert', 'good', 'health', 'lifestyle', 'say']\n"
     ]
    }
   ],
   "source": [
    "print([k for k in dictionary.keys()])\n",
    "print(\"\\n\")\n",
    "print([dictionary[k] for k in dictionary.keys()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim has a token to id method and id to token method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.token2id[\"perform\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'perform'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.id2token[25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In gensim, we can get bag of word frequencies for a document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sugar', 'bad', 'consume', 'sister', 'like', 'sugar', 'father']\n",
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 2)]\n",
      "id = 0 word = bad freq = 1\n",
      "id = 1 word = consume freq = 1\n",
      "id = 2 word = father freq = 1\n",
      "id = 3 word = like freq = 1\n",
      "id = 4 word = sister freq = 1\n",
      "id = 5 word = sugar freq = 2\n"
     ]
    }
   ],
   "source": [
    "print(docs_cleaned[0])\n",
    "bow=dictionary.doc2bow(docs_cleaned[0])\n",
    "print(bow)\n",
    "for p in bow:\n",
    "    id=p[0]\n",
    "    word=dictionary.id2token[id]\n",
    "    print(\"id = \" + str(id) + \" word = \" +  word + \" freq = \" + str(p[1]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the entire document term matrix, which is just a list of lists of word frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in docs_cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 2)]\n",
      "[(2, 1), (4, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1)]\n",
      "[(8, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1)]\n",
      "[(2, 1), (4, 1), (18, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1)]\n",
      "[(5, 1), (30, 1), (31, 1), (32, 1), (33, 1), (34, 1)]\n"
     ]
    }
   ],
   "source": [
    "for b in doc_term_matrix:\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have created our dictionary and our document term matrix, we can proceed to building the LDA model. We need to tell the program how many topics to assume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=2500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fitted model gives us the following pieces of information:\n",
    "\n",
    "    1) The word distribution for every topic.\n",
    "    2) The topic distribution for every document\n",
    "    \n",
    "The document can be a new document or a document used in the fitting process.\n",
    "  \n",
    "To see the word distribution for the most common words for for topic k:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sugar', 0.028671578),\n",
       " ('driving', 0.028627936),\n",
       " ('pressure', 0.028607666),\n",
       " ('bad', 0.028597329),\n",
       " ('consume', 0.028597329),\n",
       " ('like', 0.028597329),\n",
       " ('spends', 0.02857541),\n",
       " ('practice', 0.02857541),\n",
       " ('time', 0.02857541),\n",
       " ('dance', 0.02857541)]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.show_topic(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get all word probabilities for the top 20 most frequent words we can use this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sugar', 0.028671578),\n",
       " ('driving', 0.028627936),\n",
       " ('pressure', 0.028607666),\n",
       " ('consume', 0.028597329),\n",
       " ('bad', 0.028597329),\n",
       " ('like', 0.028597329),\n",
       " ('time', 0.02857541),\n",
       " ('spends', 0.02857541),\n",
       " ('practice', 0.02857541),\n",
       " ('lot', 0.02857541),\n",
       " ('dance', 0.02857541),\n",
       " ('around', 0.02857541),\n",
       " ('health', 0.02857525),\n",
       " ('good', 0.02857525),\n",
       " ('expert', 0.02857525),\n",
       " ('lifestyle', 0.02857525),\n",
       " ('say', 0.02857525),\n",
       " ('sister', 0.02856972),\n",
       " ('father', 0.02856972),\n",
       " ('seems', 0.028555209)]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.show_topic(0,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to check our understanding, we sum all of the probabilities over all words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0000000558793545\n",
      "1.0000000102445483\n",
      "0.9999999245628715\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "for k in range(3):\n",
    "    pvec=np.array([x[1] for x in ldamodel.show_topic(k,35)])\n",
    "    print(sum(pvec))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sugar', 0.076213938886463745),\n",
       " ('health', 0.075290907265698981),\n",
       " ('good', 0.075290907265698981),\n",
       " ('say', 0.075290907265698981),\n",
       " ('expert', 0.075290907265698981),\n",
       " ('lifestyle', 0.075290907265698981),\n",
       " ('bad', 0.018898613671765797),\n",
       " ('consume', 0.018898613671765797),\n",
       " ('like', 0.018898613671765797),\n",
       " ('sister', 0.018875413427483355)]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.show_topic(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sugar', 0.028652824915441973),\n",
       " ('consume', 0.028609078219934581),\n",
       " ('like', 0.028609078219934581),\n",
       " ('bad', 0.028609078219934581),\n",
       " ('doctor', 0.028582158402158612),\n",
       " ('blood', 0.028582158402158612),\n",
       " ('stress', 0.028582158402158612),\n",
       " ('cause', 0.028582158402158612),\n",
       " ('may', 0.028582158402158612),\n",
       " ('suggest', 0.028582158402158612)]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.show_topic(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also get the distribution by the id number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 0.028671578),\n",
       " (8, 0.028627936),\n",
       " (18, 0.028607666),\n",
       " (0, 0.028597329),\n",
       " (1, 0.028597329),\n",
       " (3, 0.028597329),\n",
       " (6, 0.02857541),\n",
       " (7, 0.02857541),\n",
       " (9, 0.02857541),\n",
       " (10, 0.02857541),\n",
       " (11, 0.02857541),\n",
       " (12, 0.02857541),\n",
       " (32, 0.02857525),\n",
       " (31, 0.02857525),\n",
       " (30, 0.02857525),\n",
       " (33, 0.02857525),\n",
       " (34, 0.02857525),\n",
       " (2, 0.02856972),\n",
       " (4, 0.02856972),\n",
       " (25, 0.028555209),\n",
       " (21, 0.028555209),\n",
       " (22, 0.028555209),\n",
       " (23, 0.028555209),\n",
       " (24, 0.028555209),\n",
       " (29, 0.028555209),\n",
       " (26, 0.028555209),\n",
       " (27, 0.028555209),\n",
       " (28, 0.028555209),\n",
       " (13, 0.02854798),\n",
       " (15, 0.02854798),\n",
       " (20, 0.02854798),\n",
       " (19, 0.02854798),\n",
       " (14, 0.02854798),\n",
       " (16, 0.02854798),\n",
       " (17, 0.02854798)]"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.get_topic_terms(0,35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we have the word frequencies for a document, we can get it's topic distibution. Remember that we have 5 documents, and doc_term_matrix[k] is the\n",
    "k-th document's word frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.042344317), (1, 0.9144799), (2, 0.043175817)]\n",
      "[(0, 0.0340439), (1, 0.93152094), (2, 0.034435146)]\n",
      "[(0, 0.033883598), (1, 0.034168925), (2, 0.9319475)]\n",
      "[(0, 0.026210178), (1, 0.94746166), (2, 0.026328174)]\n",
      "[(0, 0.04841672), (1, 0.049542442), (2, 0.90204084)]\n"
     ]
    }
   ],
   "source": [
    "for k in range(5):\n",
    "    print(ldamodel.get_document_topics(doc_term_matrix[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can get a topic distribution for a new document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0.048415147), (1, 0.049539752), (2, 0.90204513)]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdoc=[\"doctor\",\"suggest\",\"good\",\"blood\",\"health\",\"sugar\"]\n",
    "bow=dictionary.doc2bow(newdoc)\n",
    "ldamodel.get_document_topics(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
